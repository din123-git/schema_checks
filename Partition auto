-- Create or replace the stored procedure in the specified dataset
CREATE OR REPLACE PROCEDURE your_project.your_dataset.recommend_partition_and_clustering(
  input_project_id STRING,   -- Project ID containing the table to analyze
  input_dataset_id STRING,   -- Dataset ID containing the table
  input_table_id STRING      -- Table ID to analyze for optimization
)
BEGIN
  -- ======================================================================= --
  -- INPUT VALIDATION: Ensure all required parameters are provided
  -- ======================================================================= --
  IF input_project_id IS NULL OR input_dataset_id IS NULL OR input_table_id IS NULL THEN
    -- Raise error with formatted message showing input values
    SELECT ERROR(
      FORMAT("Invalid input: project_id=%t, dataset_id=%t, table_id=%t", 
             input_project_id, input_dataset_id, input_table_id)
    );
  END IF;

  -- ======================================================================= --
  -- VARIABLE DECLARATIONS: Store intermediate results and state
  -- ======================================================================= --
  DECLARE full_table_name STRING DEFAULT CONCAT(input_project_id, '.', input_dataset_id, '.', input_table_id);
  DECLARE table_region STRING DEFAULT 'US';  -- Default to US region if detection fails
  DECLARE partition_col STRING DEFAULT NULL;  -- Recommended partitioning column
  DECLARE partition_count INT64 DEFAULT NULL; -- Distinct count in partition column
  DECLARE partition_count_sql STRING;         -- Dynamic SQL for partition count
  DECLARE region_dataset STRING;              -- Formatted region string
  DECLARE column_count INT64 DEFAULT 0;       -- Count of qualifying columns
  DECLARE current_project STRING;             -- Project where procedure executes
  DECLARE table_row_count INT64 DEFAULT NULL; -- Total rows in analyzed table

  -- ======================================================================= --
  -- CONTEXT SETUP: Get current execution environment
  -- ======================================================================= --
  -- Get project ID where this procedure is running
  SET current_project = (SELECT @@project_id);

  -- ======================================================================= --
  -- STEP 0: DETECT TABLE REGION - Required for accessing INFORMATION_SCHEMA
  -- ======================================================================= --
  BEGIN
    -- Dynamic query to get table's region from metadata
    EXECUTE IMMEDIATE FORMAT("""
      SELECT COALESCE(MAX(region), 'US')  -- Fallback to US if NULL
      FROM `%s.INFORMATION_SCHEMA.TABLES`
      WHERE table_schema = @dataset 
        AND table_name = @table
    """, input_project_id)
    INTO table_region  -- Store result in variable
    USING input_dataset_id AS dataset, input_table_id AS table;  -- Parameter binding
  EXCEPTION WHEN ERROR THEN
    -- If region detection fails, use default US region
    SET table_region = 'US';
  END;

  -- Format region for INFORMATION_SCHEMA access
  SET region_dataset = CONCAT('region-', table_region);

  -- ======================================================================= --
  -- GET TABLE SIZE: Capture row count for context in recommendations
  -- ======================================================================= --
  BEGIN
    -- Execute simple count query on target table
    EXECUTE IMMEDIATE FORMAT("SELECT COUNT(*) FROM `%s`", full_table_name)
    INTO table_row_count;  -- Store result
  EXCEPTION WHEN ERROR THEN
    -- If count fails, leave as NULL (table might be empty or inaccessible)
    SET table_row_count = NULL;
  END;

  -- ======================================================================= --
  -- STEP 1: GET TABLE COLUMNS - Fetch schema metadata for analysis
  -- ======================================================================= --
  BEGIN
    -- Dynamic query to get column names and types from INFORMATION_SCHEMA
    EXECUTE IMMEDIATE FORMAT("""
      CREATE OR REPLACE TEMP TABLE table_columns AS
      SELECT 
        LOWER(column_name) AS column_name,  -- Normalize to lowercase
        LOWER(data_type) AS data_type       -- Normalize data types
      FROM `%s.%s.INFORMATION_SCHEMA.COLUMNS`  -- Region-specific schema
      WHERE table_schema = @dataset 
        AND table_name = @table
        AND data_type NOT LIKE 'ARRAY%%'   -- Exclude array columns
        AND data_type NOT LIKE 'STRUCT%%'  -- Exclude struct columns
    """, input_project_id, region_dataset)
    USING input_dataset_id AS dataset, input_table_id AS table;
  EXCEPTION WHEN ERROR THEN
    -- If schema access fails, create empty table to prevent errors
    CREATE OR REPLACE TEMP TABLE table_columns AS
    SELECT CAST(NULL AS STRING) AS column_name, CAST(NULL AS STRING) AS data_type
    WHERE FALSE;  -- Creates empty table structure
  END;

  -- ======================================================================= --
  -- STEP 2: EXTRACT QUERY LOGS - Get recent queries accessing the table
  -- ======================================================================= --
  BEGIN
    -- Query job history in current project's region
    EXECUTE IMMEDIATE FORMAT("""
      CREATE OR REPLACE TEMP TABLE query_usage AS
      SELECT 
        query,              -- SQL text of executed queries
        referenced_tables   -- Tables referenced in query
      FROM `%s.%s.INFORMATION_SCHEMA.JOBS_BY_PROJECT`
      WHERE creation_time >= TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 90 DAY)
        AND state = 'DONE'           -- Only completed jobs
        AND job_type IN ('QUERY', 'SCRIPT')  -- Include both queries and scripts
        AND EXISTS (  -- Filter to queries that reference our table
          SELECT 1
          FROM UNNEST(referenced_tables) t
          WHERE LOWER(CONCAT(t.project_id, '.', t.dataset_id, '.', t.table_id)) = @table_name
        )
    """, current_project, region_dataset)
    USING full_table_name AS table_name;  -- Bind table name parameter
  EXCEPTION WHEN ERROR THEN
    -- If job history inaccessible, create empty result set
    CREATE OR REPLACE TEMP TABLE query_usage AS
    SELECT CAST(NULL AS STRING) AS query, []  -- Empty array for referenced_tables
    WHERE FALSE;
  END;

  -- ======================================================================= --
  -- STEP 3: EXTRACT COLUMNS FROM QUERIES - Parse WHERE/JOIN/GROUP BY clauses
  -- ======================================================================= --
  CREATE OR REPLACE TEMP TABLE extracted_columns AS
  WITH clause_extractor AS (
    -- Extract clause sections using regex
    SELECT
      -- WHERE clause: Capture content until next keyword or end
      REGEXP_EXTRACT_ALL(LOWER(query), r'where\s+((?:[^)(]*(?:\([^)(]*\)[^)(]*)*)') AS where_clauses,
      -- JOIN conditions: Capture ON clause content
      REGEXP_EXTRACT_ALL(LOWER(query), r'join\s+\w+\s+on\s+((?:[^)(]*(?:\([^)(]*\)[^)(]*)*)') AS join_clauses,
      -- GROUP BY: Capture column list
      REGEXP_EXTRACT_ALL(LOWER(query), r'group\s+by\s+([^()]+(?:\([^()]*\)[^()]*)*') AS groupby_clauses
    FROM query_usage
    WHERE query IS NOT NULL  -- Skip null queries (from error cases)
  ),
  -- Extract column names from clauses
  raw_columns AS (
    -- FROM WHERE clauses
    SELECT
      -- Extract base column name (remove table aliases)
      TRIM(REGEXP_EXTRACT(col, r'^.*\.?([\w_]+)$')) AS parsed_col
    FROM clause_extractor,
      UNNEST(where_clauses) AS where_clause,          -- Unnest WHERE clauses
      UNNEST(REGEXP_EXTRACT_ALL(where_clause, r'([\w_]+(?:\.[\w_]+)?)')) AS col  -- Split conditions
    
    UNION ALL  -- Combine results
    
    -- FROM JOIN conditions
    SELECT
      TRIM(REGEXP_EXTRACT(col, r'^.*\.?([\w_]+)$')) AS parsed_col
    FROM clause_extractor,
      UNNEST(join_clauses) AS join_clause,            -- Unnest JOIN clauses
      UNNEST(REGEXP_EXTRACT_ALL(join_clause, r'([\w_]+(?:\.[\w_]+)?)')) AS col
    
    UNION ALL  -- Combine results
    
    -- FROM GROUP BY clauses
    SELECT
      TRIM(REGEXP_EXTRACT(col, r'^.*\.?([\w_]+)$')) AS parsed_col
    FROM clause_extractor,
      UNNEST(groupby_clauses) AS groupby_clause,       -- Unnest GROUP BY
      UNNEST(SPLIT(groupby_clause, ',')) AS col        -- Split comma-separated columns
  ),
  -- Filter out invalid column candidates
  filtered_columns AS (
    SELECT parsed_col
    FROM raw_columns
    WHERE parsed_col IS NOT NULL
      -- Exclude SQL keywords and literals
      AND parsed_col NOT IN (
        'null', 'true', 'false', 'current_date', 'current_timestamp',
        'current_time', 'current_datetime', 'date', 'timestamp', 'interval'
      )
      -- Ensure valid column name pattern (letters, numbers, underscores)
      AND REGEXP_CONTAINS(parsed_col, r'^[a-z_][a-z0-9_]*$')
  )
  -- Final output of candidate columns
  SELECT parsed_col
  FROM filtered_columns;

  -- ======================================================================= --
  -- STEP 4: COUNT COLUMN USAGE FREQUENCY - Identify most used columns
  -- ======================================================================= --
  CREATE OR REPLACE TEMP TABLE column_frequency AS
  SELECT
    parsed_col AS column_name,
    COUNT(*) AS usage_count  -- Count appearances in queries
  FROM extracted_columns
  WHERE parsed_col IN (SELECT column_name FROM table_columns)  -- Validate against real columns
  GROUP BY 1
  HAVING COUNT(*) >= 3  -- Minimum usage threshold to filter noise
  ORDER BY usage_count DESC;  -- Most used first

  -- Store count of qualifying columns for later decisions
  SET column_count = (SELECT COUNT(*) FROM column_frequency);

  -- ======================================================================= --
  -- STEP 5: RECOMMEND PARTITION COLUMN - Must be DATE/TIMESTAMP type
  -- ======================================================================= --
  CREATE OR REPLACE TEMP TABLE partition_recommendation AS
  SELECT
    cf.column_name,
    cf.usage_count
  FROM column_frequency cf
  JOIN table_columns tc
    ON cf.column_name = tc.column_name
  WHERE tc.data_type IN ('date', 'timestamp')  -- Only temporal partitioning
  ORDER BY usage_count DESC  -- Prefer most frequently used
  LIMIT 1;  -- Take top candidate

  -- ======================================================================= --
  -- STEP 6: ESTIMATE PARTITION COUNT - Check partition effectiveness
  -- ======================================================================= --
  -- Only proceed if we have qualifying columns
  IF column_count > 0 THEN
    -- Get top partition candidate
    SET partition_col = (SELECT column_name FROM partition_recommendation LIMIT 1);
    
    -- If partition candidate exists
    IF partition_col IS NOT NULL THEN
      -- Build dynamic SQL to count distinct values
      SET partition_count_sql = FORMAT("""
        SELECT COUNT(DISTINCT `%s`)  -- Count distinct values
        FROM `%s`                    -- From target table
        WHERE `%s` IS NOT NULL       -- Exclude nulls
      """, partition_col, full_table_name, partition_col);
      
      -- Safely execute distinct count query
      BEGIN
        EXECUTE IMMEDIATE partition_count_sql INTO partition_count;
      EXCEPTION WHEN ERROR THEN
        -- If query fails, set to NULL (column might be renamed)
        SET partition_count = NULL;
      END;
    END IF;
  END IF;

  -- ======================================================================= --
  -- STEP 7: RECOMMEND CLUSTERING COLUMNS - Up to 4 columns
  -- ======================================================================= --
  -- Initialize empty table for clustering recommendations
  CREATE OR REPLACE TEMP TABLE clustering_recommendation AS
  SELECT CAST(NULL AS STRING) AS column_name, 0 AS usage_count
  WHERE FALSE;  -- Creates empty structure
  
  -- Only find clustering columns if we have qualifying columns
  IF column_count > 0 THEN
    -- Get top columns excluding partition candidate
    CREATE OR REPLACE TEMP TABLE clustering_recommendation AS
    SELECT
      column_name,
      usage_count
    FROM column_frequency
    WHERE column_name != COALESCE(partition_col, '')  -- Exclude partition column
    ORDER BY usage_count DESC  -- Most used first
    LIMIT 4;  -- BigQuery supports max 4 clustering columns
  END IF;

  -- ======================================================================= --
  -- STEP 8: STORE RECOMMENDATIONS - Output results to permanent table
  -- ======================================================================= --
  -- Create output table if doesn't exist
  CREATE TABLE IF NOT EXISTS your_project.your_dataset.optimization_recommendation (
    project_id STRING,                         -- Analyzed project
    dataset_id STRING,                         -- Analyzed dataset
    table_id STRING,                           -- Analyzed table
    recommended_partition_column STRING,        -- Partition recommendation
    estimated_partition_count INT64,            -- Distinct partition count
    partition_notes STRING,                    -- Partition suitability assessment
    recommended_clustering_columns ARRAY<STRING>, -- Ordered clustering columns
    table_row_count INT64,                     -- Total rows in table
    generated_at TIMESTAMP,                    -- When analysis ran
    analysis_notes STRING                      -- Diagnostic messages
  );

  -- Insert recommendation results
  INSERT INTO your_project.your_dataset.optimization_recommendation
  SELECT
    input_project_id,
    input_dataset_id,
    input_table_id,
    partition_col AS recommended_partition_column,
    partition_count AS estimated_partition_count,
    -- Partition suitability assessment
    CASE
      WHEN column_count = 0 THEN 'No qualifying columns found'
      WHEN partition_col IS NULL THEN 'No suitable DATE/TIMESTAMP column found'
      WHEN partition_count > 4000 THEN 'WARNING: >4000 partitions - consider time-unit partitioning'
      WHEN partition_count < 10 THEN 'WARNING: Low partition count - may not provide benefits'
      ELSE 'Partitioning recommended'
    END AS partition_notes,
    -- Clustering columns (ordered by usage frequency)
    COALESCE(
      ARRAY(SELECT column_name FROM clustering_recommendation ORDER BY usage_count DESC),
      []  -- Empty array if no recommendations
    ) AS recommended_clustering_columns,
    table_row_count,  -- Row count captured earlier
    CURRENT_TIMESTAMP(),  -- Current timestamp
    -- Diagnostic notes for edge cases
    CASE
      WHEN (SELECT COUNT(*) FROM table_columns) = 0 THEN 'Could not access table schema'
      WHEN (SELECT COUNT(*) FROM query_usage) = 0 THEN 'No query history found in last 90 days'
      WHEN column_count = 0 THEN 'No columns met usage threshold (3+ occurrences in WHERE/JOIN/GROUP BY)'
      ELSE NULL  -- No issues
    END AS analysis_notes
  ;
END;
